{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our goal is to transform the original dataset provided from LMS into the representation of the star schema which we designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libary and dataset imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import pandas and openpyxl, with the second one done by using pip since it gave us problems locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pip\n",
    "pip.main([\"install\", \"openpyxl\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the excel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../../00-Project/datasets/2024-08-01_LMS_data_2023.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the different sheets into their respective data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the different sheets in their own dataframes. We don't do it in one cell for testing reasons. \\\n",
    "Shipment is our fact, and we identified as dimensions the information about:\n",
    "<ul>\n",
    "  <li>Carrier</li>\n",
    "  <li>Domain</li>\n",
    "  <li>Country</li>\n",
    "  <li>Service</li>\n",
    "  <li>Customer</li>\n",
    "  <li>Pickup address</li>\n",
    "  <li>Delivery address</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipment_df = pd.read_excel(xls, 'shipment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_df = pd.read_excel(xls, 'carrier')\n",
    "domain_df = pd.read_excel(xls, 'domain')\n",
    "country_df = pd.read_excel(xls, 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_df = pd.read_excel(xls, 'service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_excel(xls, 'customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickupaddress_df = pd.read_excel(xls, 'pickupaddress')\n",
    "deliveryaddress_df = pd.read_excel(xls, 'deliveryaddress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branchcode_df = pd.read_excel(xls, 'branchcode')\n",
    "branchcode_customer_translation_df = pd.read_excel(xls, 'branchcode_customer_translation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of shipment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the shipment dataframe, we make some quality checks for the shipment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert 'shipment_id' to numeric, then turn invalid values (non-numeric) to NaN\n",
    "shipment_df['shipment_id'] = pd.to_numeric(shipment_df['shipment_id'], errors='coerce')\n",
    "\n",
    "# We drop rows where 'shipment_id' is NaN\n",
    "shipment_df_clean = shipment_df.dropna(subset=['shipment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to make quality checks for the dates present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this function we check is the date is valid and within reasonale bounds\n",
    "def fix_and_check_date(date_str):\n",
    "    try:\n",
    "        date = pd.to_datetime(date_str, errors='raise')  # We attempt to convert the date\n",
    "        if date.year > 2026:  # If the year is out of bounds (beyond 2026)\n",
    "            return pd.NaT  # We mark it as NaT (invalid)\n",
    "        return date\n",
    "    except:\n",
    "        return pd.NaT  # We mark as NaT if the conversion fails\n",
    "\n",
    "# We apply the function to our date columns and we discard the time\n",
    "for col in ['created_date', 'real_delivery_date', 'real_pickup_date']:\n",
    "    shipment_df_clean[col] = shipment_df_clean[col].apply(fix_and_check_date).dt.date\n",
    "\n",
    "# We drop the rows where any date column contains NaT\n",
    "shipment_df_clean.dropna(subset=['created_date', 'real_delivery_date', 'real_pickup_date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the shipment fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we merge shipment with the domain in order to create the shipment fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge the domain and the shipment, using 'domain_id' as key\n",
    "fact_shipment = shipment_df_clean.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "\n",
    "# We rename 'name' and 'bookingstate' for easier interpretation\n",
    "fact_shipment = fact_shipment.rename(columns={'name': 'domain_name',\n",
    "                                              'bookingstate': 'booking_state'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the relevant columns to keep, in accordance with the star schema model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_shipment = fact_shipment[['shipment_id', 'customer_price', 'expected_carrier_price', \n",
    "                               'final_carrier_price', 'weight', 'shipment_type', \n",
    "                               'insurance_type', 'customer_id', 'pickupaddress_id', \n",
    "                               'deliveryaddress_id', 'service_id', 'domain_name', \n",
    "                               'pickup_date', 'delivery_date', 'real_pickup_date', \n",
    "                               'real_delivery_date', 'booking_state', 'lms_plus', \n",
    "                               'exworks_id','created_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the margin by subtracting the customer price and the final carrier price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert both columns to numeric, coercing errors (invalid parsing will be set to NaN)\n",
    "fact_shipment['customer_price'] = pd.to_numeric(fact_shipment['customer_price'], errors='coerce')\n",
    "fact_shipment['final_carrier_price'] = pd.to_numeric(fact_shipment['final_carrier_price'], errors='coerce')\n",
    "\n",
    "# We calculate margin, leaving it as NaN where values are missing\n",
    "fact_shipment['margin'] = fact_shipment['customer_price'] - fact_shipment['final_carrier_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process pickup and delivery address data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the pickup and delivery address dimensions, considering the important columns and tying them to the shipment fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the pickup dimension and we merge the domain name into it\n",
    "dim_pickup_address = pickupaddress_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "dim_pickup_address = dim_pickup_address.rename(columns={'name': 'domain_name'})\n",
    "\n",
    "# We keep the columns present in the defined star schema\n",
    "dim_pickup_address = dim_pickup_address[['pickupaddress_id', 'created_date', 'domain_name', 'country_id', 'postal_code', 'city']]\n",
    "dim_pickup_address = dim_pickup_address.rename(columns={'pickupaddress_id': 'pickup_address_id'})\n",
    "\n",
    "# We create the pickup dimension and we merge the domain name into it\n",
    "dim_delivery_address = deliveryaddress_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "dim_delivery_address = dim_delivery_address.rename(columns={'name': 'domain_name'})\n",
    "\n",
    "# We keep the columns present in the defined star schema\n",
    "dim_delivery_address = dim_delivery_address[['deliveryaddress_id', 'created_date', 'domain_name', 'country_id', 'postal_code', 'city']]\n",
    "dim_delivery_address = dim_delivery_address.rename(columns={'deliveryaddress_id': 'delivery_address_id'})\n",
    "\n",
    "# We rename the columns in the shipment fact table for better understanding\n",
    "fact_shipment = fact_shipment.rename(columns={'pickupaddress_id': 'pickup_address_id', \n",
    "                                                'deliveryaddress_id': 'delivery_address_id'})\n",
    "\n",
    "# We ensure both dimensions contain only the date parts\n",
    "dim_delivery_address['created_date'] = pd.to_datetime(dim_delivery_address['created_date'], errors='coerce').dt.date\n",
    "dim_pickup_address['created_date'] = pd.to_datetime(dim_pickup_address['created_date'], errors='coerce').dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process customer data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the customer dimensions; we handle it's relationships with the branchcodes, other than the master relationships.\n",
    "\n",
    "Customers are organized in a hierarchical structure where:\n",
    "<ul>\n",
    "  <li>Master Accounts are identified when a customer's sequence number matches their structure number</li>\n",
    "  <li>Industry Classifications are assigned through branchcodes:</li>\n",
    "  <ul>\n",
    "    <li>Each customer can have multiple branchcodes</li>\n",
    "    <li>Only the first/primary branchcode is used as the main industry, meaning the main activity that the customer does. We will consider it in order to have a 1 to n relationship</li>\n",
    "    <li>Each branchcode has both a specific industry name and a broader sector classification</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we handle the customer and the industries; we merge the branchcode and the root branch (which we refer to using the sector). \\\n",
    "We get the main industry for each customer by using the first branchcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We join the customer with the branchcode table to get both the specific industry name and its root info\n",
    "customer_industries = (\n",
    "    customer_df.merge(\n",
    "        # First we get the translation table to link customers to branchcodes\n",
    "        branchcode_customer_translation_df,\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Then we get the branchcode information, including the branch name\n",
    "    .merge(\n",
    "        branchcode_df[['branchcode_id', 'branch_name', 'root_branch_id']],\n",
    "        on='branchcode_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # At last, we get the root branch name by joining branchcode table again\n",
    "    .merge(\n",
    "        branchcode_df[['branchcode_id', 'branch_name']],\n",
    "        left_on='root_branch_id',\n",
    "        right_on='branchcode_id',\n",
    "        how='left',\n",
    "        suffixes=('', '_root')\n",
    "    )\n",
    "    # We sort by translation ID to ensure that the main industry comes first\n",
    "    .sort_values('branchcode_customer_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get only the main industry name and its root branch name for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_industry_info = customer_industries.groupby('customer_id').agg({\n",
    "    'branch_name': 'first',          # We get the main industry name\n",
    "    'branch_name_root': 'first'      # We get the root branch name\n",
    "}).reset_index()\n",
    "\n",
    "# Now we can create the customer dimension with all the information \n",
    "dim_customer = (\n",
    "    customer_df\n",
    "    # Here we merge the industry information\n",
    "    .merge(\n",
    "        customer_industry_info[['customer_id', 'branch_name', 'branch_name_root']], \n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Here we merge the domain information\n",
    "    .merge(\n",
    "        domain_df[['domain_id', 'name']],\n",
    "        on='domain_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Lastly we rename some of the columns in order to be more descriptive\n",
    "    .rename(columns={\n",
    "        'name': 'domain_name',\n",
    "        'sequencenumber': 'sequence_number',\n",
    "        'structurenumber': 'structure_number',\n",
    "        'branch_name': 'main_industry_name',\n",
    "        'branch_name_root': 'industry_sector_name'\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step, we add for each customer a column displaying if it is the 'master', meaning the main entity of the sequence and structure number hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer['is_master'] = dim_customer['sequence_number'] == dim_customer['structure_number']\n",
    "dim_customer = dim_customer[[\n",
    "    'customer_id', 'created_date', 'domain_name', \n",
    "    'main_industry_name', 'industry_sector_name',\n",
    "    'segmentation', 'sequence_number', 'structure_number', \n",
    "    'is_master'\n",
    "]]\n",
    "dim_customer['created_date'] = pd.to_datetime(dim_customer['created_date'], errors='coerce').dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dates data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the date dimensions; first we do some data quality check, aster which we create the dimensions and tie them to the fact table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we extract the relevant date columns from shipment\n",
    "date_columns = ['created_date', 'pickup_date', 'real_pickup_date', 'delivery_date', 'real_delivery_date']\n",
    "\n",
    "# Here we remove time from dates that include time\n",
    "for col in ['created_date', 'real_delivery_date', 'real_pickup_date']:\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce').dt.date  \n",
    "\n",
    "# Here we process each date column separately in order to avoid memory overload and crashing\n",
    "date_dim = pd.DataFrame()\n",
    "\n",
    "for col in date_columns:\n",
    "    # Here we convert each date column to datetime and remove invalid dates\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce')\n",
    "    \n",
    "    # Here we combine the current date column into the date dimension, avoiding duplication\n",
    "    new_dates = fact_shipment[[col]].drop_duplicates().dropna().rename(columns={col: 'full_date'})\n",
    "    date_dim = pd.concat([date_dim, new_dates]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Here we ensure full_date is only date and we create month, quarter, and year columns\n",
    "date_dim['full_date'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.date  \n",
    "date_dim['year'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.year      \n",
    "date_dim['month'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.month    \n",
    "date_dim['quarter'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.quarter\n",
    "date_dim['date_id'] = date_dim.index + 1  # Create incremental date IDs\n",
    "\n",
    "# Here we replace date columns in the shipment table with corresponding date IDs, processing them one at a time\n",
    "for col in date_columns:\n",
    "    # Here we ensure both the fact_shipment column and the full_date column are in the same datetime format\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce').dt.date\n",
    "    date_dim['full_date'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.date\n",
    "    \n",
    "    # Here we merge fact_shipment with the date dimension to assign date IDs\n",
    "    fact_shipment = fact_shipment.merge(date_dim[['full_date', 'date_id']], left_on=col, right_on='full_date', how='left')\n",
    "    \n",
    "    # Here we rename the new column and drop the redundant 'full_date' column\n",
    "    fact_shipment = fact_shipment.rename(columns={'date_id': f'{col}_id'}).drop(columns=['full_date'])\n",
    "\n",
    "# Lastly, we drop the original date columns as we now have the date IDs in place\n",
    "fact_shipment = fact_shipment.drop(columns=date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process service dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the service dimension; we merge the domain information into the service and handle the naming alongside the importanct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we merge domain name into the service dimension table, adding suffixes to distinguish between the columns\n",
    "dim_service = service_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left', suffixes=('_service', '_domain'))\n",
    "\n",
    "# Here we rename the different columns for clarity and formatting\n",
    "dim_service = dim_service.rename(columns={'name_domain': 'domain_name'})\n",
    "dim_service = dim_service.rename(columns={'name_service': 'name'})\n",
    "dim_service = dim_service.rename(columns={'servicetype': 'service_type',\n",
    "                                            'transporttype': 'transport_type'})\n",
    "\n",
    "# Here we keep only the relevant columns\n",
    "dim_service = dim_service[['service_id', 'created_date', 'name', 'service_type', 'transport_type', 'carrier_id', 'domain_name']]\n",
    "\n",
    "# Here we ensure the date doens't contain the time\n",
    "dim_service['created_date'] = pd.to_datetime(dim_service['created_date'], errors='coerce').dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process carrier and country dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the carrier and country dimensions, merging the carrier with the domain information and keeping the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we merge the domain name into the service dimension table, adding suffixes to distinguish between columns\n",
    "dim_carrier = carrier_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left', suffixes=('_carrier', '_domain'))\n",
    "\n",
    "# Here we rename some of the columns for clarity and formatting reasons\n",
    "dim_carrier = dim_carrier.rename(columns={'name_carrier': 'name',\n",
    "                                            'name_domain': 'domain_name'})\n",
    "\n",
    "# Here we filter the important columns\n",
    "dim_carrier = dim_carrier[['carrier_id', 'name', 'created_date', 'domain_name']]\n",
    "\n",
    "# Here we ensure that created_date contains only the date\n",
    "dim_carrier['created_date'] = pd.to_datetime(dim_carrier['created_date'], errors='coerce').dt.date  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the country dimension, filtering the needed columns\n",
    "dim_country = country_df[['country_id', 'name', 'isocountrycode', 'continent', 'EU']]\n",
    "\n",
    "# Here we rename the 'isocountrycode' column for clarity\n",
    "dim_country = dim_country.rename(columns={'isocountrycode': 'iso_country_code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new start schema dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last operation left to do is to merge toeghter the fact table and different dimensions into a single excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('../../../00-Project/datasets/star_schema_dataset.xlsx', engine='xlsxwriter') as writer:\n",
    "    # The Fact table\n",
    "    fact_shipment.to_excel(writer, sheet_name='fact_shipment', index=False)\n",
    "    \n",
    "    # The Dimension tables\n",
    "    dim_customer.to_excel(writer, sheet_name='dim_customer', index=False)\n",
    "    dim_delivery_address.to_excel(writer, sheet_name='dim_delivery_address', index=False)\n",
    "    dim_pickup_address.to_excel(writer, sheet_name='dim_pickup_address', index=False)\n",
    "    date_dim.to_excel(writer, sheet_name='dim_date', index=False)\n",
    "    dim_service.to_excel(writer, sheet_name='dim_service', index=False)\n",
    "    dim_carrier.to_excel(writer, sheet_name='dim_carrier', index=False)\n",
    "    dim_country.to_excel(writer, sheet_name='dim_country', index=False)\n",
    "\n",
    "print(\"Star schema transformation with domain names included completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
