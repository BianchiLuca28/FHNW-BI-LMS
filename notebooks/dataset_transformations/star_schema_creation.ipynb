{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "pip.main([\"install\", \"openpyxl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "file_path = \"../datasets/2024-08-01_LMS_data_2023.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the different sheets into their respective data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant sheets\n",
    "shipment_df = pd.read_excel(xls, 'shipment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_df = pd.read_excel(xls, 'carrier')\n",
    "domain_df = pd.read_excel(xls, 'domain')\n",
    "country_df = pd.read_excel(xls, 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_df = pd.read_excel(xls, 'service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_excel(xls, 'customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickupaddress_df = pd.read_excel(xls, 'pickupaddress')\n",
    "deliveryaddress_df = pd.read_excel(xls, 'deliveryaddress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'shipment_id' to numeric, invalid values (non-numeric) will become NaN\n",
    "shipment_df['shipment_id'] = pd.to_numeric(shipment_df['shipment_id'], errors='coerce')\n",
    "\n",
    "# Drop rows where 'shipment_id' is NaN (i.e., rows with non-numerical IDs)\n",
    "shipment_df_clean = shipment_df.dropna(subset=['shipment_id'])\n",
    "\n",
    "# (Optional) If you want to cast the shipment_id back to integers:\n",
    "shipment_df_clean['shipment_id'] = shipment_df_clean['shipment_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the date is valid and within the acceptable range\n",
    "def fix_and_check_date(date_str):\n",
    "    try:\n",
    "        date = pd.to_datetime(date_str, errors='raise')  # Attempt to convert the date\n",
    "        if date.year > 2262:  # If the year is out of bounds (beyond 2262)\n",
    "            return pd.NaT  # Mark it as NaT (invalid)\n",
    "        return date\n",
    "    except:\n",
    "        return pd.NaT  # Mark as NaT if conversion fails\n",
    "\n",
    "# Apply the function to your date columns and discard time\n",
    "for col in ['created_date', 'real_delivery_date', 'real_pickup_date']:\n",
    "    shipment_df_clean[col] = shipment_df_clean[col].apply(fix_and_check_date).dt.date  # Keep only the date part\n",
    "\n",
    "# Drop rows where any date column contains NaT (invalid or out-of-bounds dates)\n",
    "shipment_df_clean.dropna(subset=['created_date', 'real_delivery_date', 'real_pickup_date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the shipment data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge domain name into the shipment fact table\n",
    "fact_shipment = shipment_df_clean.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "fact_shipment = fact_shipment.rename(columns={'name': 'domain_name',\n",
    "                                              'bookingstate': 'booking_state'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep relevant columns and calculate margin\n",
    "fact_shipment = fact_shipment[['shipment_id', 'customer_price', 'expected_carrier_price', \n",
    "                               'final_carrier_price', 'weight', 'shipment_type', \n",
    "                               'insurance_type', 'customer_id', 'pickupaddress_id', \n",
    "                               'deliveryaddress_id', 'service_id', 'domain_name', \n",
    "                               'pickup_date', 'delivery_date', 'real_pickup_date', \n",
    "                               'real_delivery_date', 'booking_state', 'lms_plus', \n",
    "                               'exworks_id','created_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both columns to numeric, coercing errors (invalid parsing will be set to NaN)\n",
    "fact_shipment['customer_price'] = pd.to_numeric(fact_shipment['customer_price'], errors='coerce')\n",
    "fact_shipment['final_carrier_price'] = pd.to_numeric(fact_shipment['final_carrier_price'], errors='coerce')\n",
    "\n",
    "# Calculate margin, leaving it as NaN where values are missing\n",
    "fact_shipment['margin'] = fact_shipment['customer_price'] - fact_shipment['final_carrier_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipment_df = None\n",
    "shipment_df_clean = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process pickup and delivery address data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the Pickup Address Dimension\n",
    "# Merge the domain name into the pickup address dimension\n",
    "dim_pickup_address = pickupaddress_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "dim_pickup_address = dim_pickup_address.rename(columns={'name': 'domain_name'})\n",
    "\n",
    "# Keep relevant columns for pickup address dimension\n",
    "dim_pickup_address = dim_pickup_address[['pickupaddress_id', 'created_date', 'domain_name', 'country_id', 'postal_code', 'city']]\n",
    "dim_pickup_address = dim_pickup_address.rename(columns={'pickupaddress_id': 'pickup_address_id'})\n",
    "\n",
    "# Step 2: Create the Delivery Address Dimension\n",
    "# Merge the domain name into the delivery address dimension\n",
    "dim_delivery_address = deliveryaddress_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "dim_delivery_address = dim_delivery_address.rename(columns={'name': 'domain_name'})\n",
    "\n",
    "# Keep relevant columns for delivery address dimension\n",
    "dim_delivery_address = dim_delivery_address[['deliveryaddress_id', 'created_date', 'domain_name', 'country_id', 'postal_code', 'city']]\n",
    "dim_delivery_address = dim_delivery_address.rename(columns={'deliveryaddress_id': 'delivery_address_id'})\n",
    "\n",
    "# Rename columns in the shipment fact table for clarity\n",
    "fact_shipment = fact_shipment.rename(columns={'pickupaddress_id': 'pickup_address_id', \n",
    "                                                'deliveryaddress_id': 'delivery_address_id'})\n",
    "\n",
    "dim_delivery_address['created_date'] = pd.to_datetime(dim_delivery_address['created_date'], errors='coerce').dt.date  # Ensure created_date contains only the date\n",
    "dim_pickup_address['created_date'] = pd.to_datetime(dim_pickup_address['created_date'], errors='coerce').dt.date  # Ensure created_date contains only the date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process customer data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branchcode_df = pd.read_excel(xls, 'branchcode')\n",
    "branchcode_customer_translation_df = pd.read_excel(xls, 'branchcode_customer_translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get main and all industries for each customer\n",
    "customer_industries = (\n",
    "    customer_df.merge(branchcode_customer_translation_df, on='customer_id', how='left')\n",
    "    .merge(branchcode_df[['branchcode_id', 'branch_name', 'root_branch_id']], on='branchcode_id', how='left')\n",
    ")\n",
    "\n",
    "# Step 2: Create main and additional industries\n",
    "customer_industry_info = customer_industries.groupby('customer_id').agg({\n",
    "    'branch_name': lambda x: [i for i in x if isinstance(i, str)],  # Filter out NaN values\n",
    "    'root_branch_id': 'first'  \n",
    "}).reset_index()\n",
    "\n",
    "customer_industry_info['main_industry'] = customer_industry_info['branch_name'].apply(lambda x: x[0] if x else None)\n",
    "customer_industry_info['all_industries'] = customer_industry_info['branch_name'].apply(lambda x: '|'.join(x) if x else None)\n",
    "\n",
    "# Step 3: Create customer dimension with all information\n",
    "dim_customer = (\n",
    "    customer_df\n",
    "    .merge(customer_industry_info[['customer_id', 'main_industry', 'all_industries', 'root_branch_id']], \n",
    "           on='customer_id', how='left')\n",
    "    .merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "    .rename(columns={\n",
    "        'name': 'domain_name',\n",
    "        'sequencenumber': 'sequence_number',\n",
    "        'structurenumber': 'structure_number'\n",
    "    })\n",
    ")\n",
    "\n",
    "# Step 4: Add master status and format\n",
    "dim_customer['is_master'] = dim_customer['sequence_number'] == dim_customer['structure_number']\n",
    "dim_customer = dim_customer[[\n",
    "    'customer_id', 'created_date', 'domain_name', \n",
    "    'main_industry', 'all_industries', 'root_branch_id',\n",
    "    'segmentation', 'sequence_number', 'structure_number', \n",
    "    'is_master'\n",
    "]]\n",
    "dim_customer['created_date'] = pd.to_datetime(dim_customer['created_date'], errors='coerce').dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dates data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract relevant date columns from shipment\n",
    "date_columns = ['created_date', 'pickup_date', 'real_pickup_date', 'delivery_date', 'real_delivery_date']\n",
    "\n",
    "# Step 2: Remove time from dates that include time (created_date, real_delivery_date, real_pickup_date)\n",
    "for col in ['created_date', 'real_delivery_date', 'real_pickup_date']:\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce').dt.date  # Keep only the date component\n",
    "\n",
    "# Step 3: Process each date column separately (to avoid memory overload during concatenation)\n",
    "date_dim = pd.DataFrame()  # Initialize an empty DataFrame for the date dimension\n",
    "\n",
    "for col in date_columns:\n",
    "    # Convert each date column to datetime and remove invalid dates\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce')\n",
    "    \n",
    "    # Combine the current date column into the date dimension, avoiding duplication\n",
    "    new_dates = fact_shipment[[col]].drop_duplicates().dropna().rename(columns={col: 'full_date'})\n",
    "    date_dim = pd.concat([date_dim, new_dates]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Step 4: Ensure full_date is only date (no time) and create month, quarter, and year columns\n",
    "date_dim['full_date'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.date  # Ensure full_date contains only the date\n",
    "date_dim['year'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.year      # Extract year\n",
    "date_dim['month'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.month    # Extract month\n",
    "date_dim['quarter'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.quarter  # Extract quarter\n",
    "date_dim['date_id'] = date_dim.index + 1  # Create incremental date IDs\n",
    "\n",
    "# Step 5: Replace date columns in the shipment table with corresponding date IDs (process one at a time)\n",
    "for col in date_columns:\n",
    "    # Ensure both the fact_shipment column and the full_date column are in the same datetime format\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce').dt.date  # Ensure it is a date, not datetime\n",
    "    date_dim['full_date'] = pd.to_datetime(date_dim['full_date'], errors='coerce').dt.date\n",
    "    \n",
    "    # Merge fact_shipment with the date dimension to assign date IDs\n",
    "    fact_shipment = fact_shipment.merge(date_dim[['full_date', 'date_id']], left_on=col, right_on='full_date', how='left')\n",
    "    \n",
    "    # Rename the new column and drop the redundant 'full_date' column\n",
    "    fact_shipment = fact_shipment.rename(columns={'date_id': f'{col}_id'}).drop(columns=['full_date'])\n",
    "\n",
    "# Step 6: Drop the original date columns as we now have the date IDs in place\n",
    "fact_shipment = fact_shipment.drop(columns=date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process service dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge domain name into the service dimension table, adding suffixes to distinguish between columns\n",
    "dim_service = service_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left', suffixes=('_service', '_domain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the domain 'name_domain' column to 'domain_name'\n",
    "dim_service = dim_service.rename(columns={'name_domain': 'domain_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the domain 'name_service' column to 'name'\n",
    "dim_service = dim_service.rename(columns={'name_service': 'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_service = dim_service.rename(columns={'servicetype': 'service_type',\n",
    "                                            'transporttype': 'transport_type'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep relevant columns\n",
    "dim_service = dim_service[['service_id', 'created_date', 'name', 'service_type', 'transport_type', 'carrier_id', 'domain_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_service['created_date'] = pd.to_datetime(dim_service['created_date'], errors='coerce').dt.date  # Ensure created_date contains only the date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process carrier and country dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge domain name into the service dimension table, adding suffixes to distinguish between columns\n",
    "dim_carrier = carrier_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left', suffixes=('_carrier', '_domain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_carrier = dim_carrier.rename(columns={'name_carrier': 'name',\n",
    "                                            'name_domain': 'domain_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the carrier dimension table\n",
    "dim_carrier = dim_carrier[['carrier_id', 'name', 'created_date', 'domain_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_carrier['created_date'] = pd.to_datetime(dim_carrier['created_date'], errors='coerce').dt.date  # Ensure created_date contains only the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_country = country_df[['country_id', 'name', 'isocountrycode', 'continent', 'EU']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_country = dim_country.rename(columns={'isocountrycode': 'iso_country_code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new start schema dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to a new Excel file representing the star schema with domain names\n",
    "with pd.ExcelWriter('../datasets/star_schema_dataset.xlsx', engine='xlsxwriter') as writer:\n",
    "    # Fact table\n",
    "    fact_shipment.to_excel(writer, sheet_name='fact_shipment', index=False)\n",
    "    \n",
    "    # Dimension tables\n",
    "    dim_customer.to_excel(writer, sheet_name='dim_customer', index=False)\n",
    "    dim_delivery_address.to_excel(writer, sheet_name='dim_delivery_address', index=False)\n",
    "    dim_pickup_address.to_excel(writer, sheet_name='dim_pickup_address', index=False)\n",
    "    date_dim.to_excel(writer, sheet_name='dim_date', index=False)\n",
    "    dim_service.to_excel(writer, sheet_name='dim_service', index=False)\n",
    "    dim_carrier.to_excel(writer, sheet_name='dim_carrier', index=False)\n",
    "    dim_country.to_excel(writer, sheet_name='dim_country', index=False)\n",
    "\n",
    "print(\"Star schema transformation with domain names included completed and saved to 'star_schema_with_domain_names.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
