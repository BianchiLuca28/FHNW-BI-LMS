{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our goal is to transform the original dataset provided from LMS into the representation of the star schema which we designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libary and dataset imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import pandas and openpyxl, with the second one done by using pip since it gave us problems locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Defaulting to user installation because normal site-packages is not writeable\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Defaulting to user installation because normal site-packages is not writeable\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: openpyxl in /home/stas/.local/lib/python3.13/site-packages (3.1.5)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: openpyxl in /home/stas/.local/lib/python3.13/site-packages (3.1.5)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: et-xmlfile in /home/stas/.local/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: et-xmlfile in /home/stas/.local/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pip\n",
    "pip.main([\"install\", \"openpyxl\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the excel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../../00-Project/datasets/2024-08-01_LMS_data_2023.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the different sheets into their respective data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the different sheets in their own dataframes. We don't do it in one cell for testing reasons. \\\n",
    "Shipment is our fact, and we identified as dimensions the information about:\n",
    "<ul>\n",
    "  <li>Carrier</li>\n",
    "  <li>Domain</li>\n",
    "  <li>Country</li>\n",
    "  <li>Service</li>\n",
    "  <li>Customer</li>\n",
    "  <li>Pickup address</li>\n",
    "  <li>Delivery address</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipment_df = pd.read_excel(xls, 'shipment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_df = pd.read_excel(xls, 'carrier')\n",
    "domain_df = pd.read_excel(xls, 'domain')\n",
    "country_df = pd.read_excel(xls, 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_df = pd.read_excel(xls, 'service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_excel(xls, 'customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickupaddress_df = pd.read_excel(xls, 'pickupaddress')\n",
    "deliveryaddress_df = pd.read_excel(xls, 'deliveryaddress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "branchcode_df = pd.read_excel(xls, 'branchcode')\n",
    "branchcode_customer_translation_df = pd.read_excel(xls, 'branchcode_customer_translation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of shipment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the shipment dataframe, we make some quality checks for the shipment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert 'shipment_id' to numeric, then turn invalid values (non-numeric) to NaN\n",
    "shipment_df['shipment_id'] = pd.to_numeric(shipment_df['shipment_id'], errors='coerce')\n",
    "\n",
    "# We drop rows where 'shipment_id' is NaN\n",
    "shipment_df_clean = shipment_df.dropna(subset=['shipment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the shipment fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we merge shipment with the domain in order to create the shipment fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge the domain and the shipment, using 'domain_id' as key\n",
    "fact_shipment = shipment_df_clean.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "\n",
    "# We rename 'name' and 'bookingstate' for easier interpretation\n",
    "fact_shipment = fact_shipment.rename(columns={'name': 'domain_name',\n",
    "                                              'bookingstate': 'booking_state'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the relevant columns to keep, in accordance with the star schema model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_shipment = fact_shipment[['shipment_id', 'customer_price', 'expected_carrier_price', \n",
    "                               'final_carrier_price', 'weight', 'shipment_type', \n",
    "                               'insurance_type', 'customer_id', 'pickupaddress_id', \n",
    "                               'deliveryaddress_id', 'service_id', 'domain_name', \n",
    "                               'pickup_date', 'delivery_date', 'real_pickup_date', \n",
    "                               'real_delivery_date', 'booking_state', 'lms_plus', \n",
    "                               'exworks_id','created_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the margin of LMS on the shipments. In the documentation, they calculate the margin in the following way:\n",
    "- customer_price - coalesce(final_carrier_price, expected_carrier_price)\n",
    "\n",
    "Which means that it's the result of customer price minus the final carrier price. If the latter is not present, the expected carrier price will be used.\n",
    "\n",
    "We then set margin to NaN in the following situations:\n",
    "- when the shipment has been cancelled\n",
    "- when the shipment is missing price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert all price columns to numeric, coercing errors to NaN\n",
    "fact_shipment['customer_price'] = pd.to_numeric(fact_shipment['customer_price'], errors='coerce')\n",
    "fact_shipment['final_carrier_price'] = pd.to_numeric(fact_shipment['final_carrier_price'], errors='coerce')\n",
    "fact_shipment['expected_carrier_price'] = pd.to_numeric(fact_shipment['expected_carrier_price'], errors='coerce')\n",
    "\n",
    "# Here we calculate carrier price using coalesce logic - use final_carrier_price if available, otherwise expected_carrier_price\n",
    "carrier_price = fact_shipment['final_carrier_price'].fillna(fact_shipment['expected_carrier_price'])\n",
    "\n",
    "# Here we calculate margin\n",
    "fact_shipment['margin'] = fact_shipment['customer_price'] - carrier_price\n",
    "\n",
    "# Here we set margin to NaN for cancelled shipments and for missing prices\n",
    "fact_shipment.loc[fact_shipment['booking_state'] > 12, 'margin'] = np.nan  \n",
    "fact_shipment.loc[fact_shipment['customer_price'].isna() | carrier_price.isna(), 'margin'] = np.nan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process pickup and delivery address data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the pickup and delivery address dimensions, considering the important columns and tying them to the shipment fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the pickup dimension and we merge the domain name into it\n",
    "dim_pickup_address = pickupaddress_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "dim_pickup_address = dim_pickup_address.rename(columns={'name': 'domain_name'})\n",
    "\n",
    "# We keep the columns present in the defined star schema\n",
    "dim_pickup_address = dim_pickup_address[['pickupaddress_id', 'created_date', 'domain_name', 'country_id', 'postal_code', 'city']]\n",
    "dim_pickup_address = dim_pickup_address.rename(columns={'pickupaddress_id': 'pickup_address_id'})\n",
    "\n",
    "# We create the pickup dimension and we merge the domain name into it\n",
    "dim_delivery_address = deliveryaddress_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left')\n",
    "dim_delivery_address = dim_delivery_address.rename(columns={'name': 'domain_name'})\n",
    "\n",
    "# We keep the columns present in the defined star schema\n",
    "dim_delivery_address = dim_delivery_address[['deliveryaddress_id', 'created_date', 'domain_name', 'country_id', 'postal_code', 'city']]\n",
    "dim_delivery_address = dim_delivery_address.rename(columns={'deliveryaddress_id': 'delivery_address_id'})\n",
    "\n",
    "# We rename the columns in the shipment fact table for better understanding\n",
    "fact_shipment = fact_shipment.rename(columns={'pickupaddress_id': 'pickup_address_id', \n",
    "                                                'deliveryaddress_id': 'delivery_address_id'})\n",
    "\n",
    "# Convert datetime while preserving the time component\n",
    "dim_delivery_address['created_date'] = pd.to_datetime(dim_delivery_address['created_date'], errors='coerce')\n",
    "dim_pickup_address['created_date'] = pd.to_datetime(dim_pickup_address['created_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process customer data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the customer dimensions; we handle it's relationships with the branchcodes, other than the master relationships.\n",
    "\n",
    "Customers are organized in a hierarchical structure where:\n",
    "<ul>\n",
    "  <li>Master Accounts are identified when a customer's sequence number matches their structure number</li>\n",
    "  <li>Industry Classifications are assigned through branchcodes:</li>\n",
    "  <ul>\n",
    "    <li>Each customer can have multiple branchcodes</li>\n",
    "    <li>Only the first/primary branchcode is used as the main industry, meaning the main activity that the customer does. We will consider it in order to have a 1 to n relationship</li>\n",
    "    <li>Each branchcode has both a specific industry name and a broader sector classification</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we handle the customer and the industries; we merge the branchcode and the root branch (which we refer to using the sector). \\\n",
    "We get the main industry for each customer by using the first branchcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We join the customer with the branchcode table to get both the specific industry name and its root info\n",
    "customer_industries = (\n",
    "    customer_df.merge(\n",
    "        # First we get the translation table to link customers to branchcodes\n",
    "        branchcode_customer_translation_df,\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Then we get the branchcode information, including the branch name\n",
    "    .merge(\n",
    "        branchcode_df[['branchcode_id', 'branch_name', 'root_branch_id']],\n",
    "        on='branchcode_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # At last, we get the root branch name by joining branchcode table again\n",
    "    .merge(\n",
    "        branchcode_df[['branchcode_id', 'branch_name']],\n",
    "        left_on='root_branch_id',\n",
    "        right_on='branchcode_id',\n",
    "        how='left',\n",
    "        suffixes=('', '_root')\n",
    "    )\n",
    "    # We sort by translation ID to ensure that the main industry comes first\n",
    "    .sort_values('branchcode_customer_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get only the main industry name and its root branch name for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_industry_info = customer_industries.groupby('customer_id').agg({\n",
    "    'branch_name': 'first',          # We get the main industry name\n",
    "    'branch_name_root': 'first'      # We get the root branch name\n",
    "}).reset_index()\n",
    "\n",
    "# Now we can create the customer dimension with all the information \n",
    "dim_customer = (\n",
    "    customer_df\n",
    "    # Here we merge the industry information\n",
    "    .merge(\n",
    "        customer_industry_info[['customer_id', 'branch_name', 'branch_name_root']], \n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Here we merge the domain information\n",
    "    .merge(\n",
    "        domain_df[['domain_id', 'name']],\n",
    "        on='domain_id',\n",
    "        how='left'\n",
    "    )\n",
    "    # Lastly we rename some of the columns in order to be more descriptive\n",
    "    .rename(columns={\n",
    "        'name': 'domain_name',\n",
    "        'sequencenumber': 'sequence_number',\n",
    "        'structurenumber': 'structure_number',\n",
    "        'branch_name': 'main_industry_name',\n",
    "        'branch_name_root': 'industry_sector_name'\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step, we add for each customer a column displaying if it is the 'master', meaning the main entity of the sequence and structure number hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer['is_master'] = dim_customer['sequence_number'] == dim_customer['structure_number']\n",
    "dim_customer = dim_customer[[\n",
    "    'customer_id', 'created_date', 'domain_name', \n",
    "    'main_industry_name', 'industry_sector_name',\n",
    "    'segmentation', 'sequence_number', 'structure_number', \n",
    "    'is_master'\n",
    "]]\n",
    "\n",
    "# Convert to datetime while preserving time if it exists\n",
    "dim_customer['created_date'] = pd.to_datetime(dim_customer['created_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dates data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the date dimension define through primary keys how it's tied to the shipment fact. \\\n",
    "Since in a previous iteration we were dropping rows where one of the date columns was NaT, and it was resulting in information loss for some categories of service type, we decided to take a different approach.\n",
    "\n",
    "We are going to use a sentinel date which will have NaT in the date dimension, and assign this date to all the values in the shipment where a date is not present, instead of outright dropping them. This way we can preserve information while handling the problematic values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we define the date columns present in shipment and we initialize the date dimension with the sentinel date, to which we give ID 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We group the relevant date columns\n",
    "date_columns = ['created_date', 'pickup_date', 'real_pickup_date', 'delivery_date', 'real_delivery_date']\n",
    "\n",
    "# We initialize the date dimension with the missing date entry\n",
    "date_dim = pd.DataFrame({\n",
    "    'full_date': [pd.NaT], # we use NaT for the missing date\n",
    "    'year': [np.nan],\n",
    "    'month': [np.nan],\n",
    "    'quarter': [np.nan],\n",
    "    'is_missing': [True]\n",
    "})\n",
    "\n",
    "date_dim['date_id'] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process the values in each column, find the valid ones, create a dataframe with the other derived features and then append it to the date dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We process each date column\n",
    "for col in date_columns:\n",
    "    # We convert to datetime, keeping invalid as NaT\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce')\n",
    "    \n",
    "    # W get unique valid dates\n",
    "    valid_dates = fact_shipment[col].dropna().unique()\n",
    "    \n",
    "    if len(valid_dates) > 0:\n",
    "        new_dates = pd.DataFrame({\n",
    "            'full_date': valid_dates,  # Keep the full datetime\n",
    "            'year': [d.year for d in valid_dates],\n",
    "            'month': [d.month for d in valid_dates],\n",
    "            'quarter': [(d.month-1)//3 + 1 for d in valid_dates],\n",
    "            'is_missing': False\n",
    "        })\n",
    "        \n",
    "        # We append the result to date dimension dropping duplicates\n",
    "        date_dim = pd.concat([date_dim, new_dates]).drop_duplicates(subset=['full_date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we assign IDs to the date dimension, reserving value 0 for the sentinel date, after which we update the fact table with the proper IDs of each date column. At last, we drop the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign proper IDs\n",
    "date_dim.loc[date_dim['is_missing'] == False, 'date_id'] = range(1, len(date_dim[date_dim['is_missing'] == False]) + 1)\n",
    "\n",
    "# Update fact table with date IDs\n",
    "for col in date_columns:\n",
    "    # Convert dates to datetime maintaining the time component\n",
    "    fact_shipment[col] = pd.to_datetime(fact_shipment[col], errors='coerce')\n",
    "    \n",
    "    # Create a mapping series that's null for missing dates\n",
    "    fact_shipment[f'{col}_date'] = fact_shipment[col]\n",
    "    \n",
    "    # Merge with date dimension to get IDs\n",
    "    fact_shipment = fact_shipment.merge(\n",
    "        date_dim[['full_date', 'date_id']],\n",
    "        left_on=f'{col}_date',\n",
    "        right_on='full_date',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing date_ids with 0 (our missing date ID)\n",
    "    fact_shipment[f'{col}_id'] = fact_shipment['date_id'].fillna(0)\n",
    "    \n",
    "    # Drop the not needed columns\n",
    "    fact_shipment = fact_shipment.drop(columns=[col, f'{col}_date', 'full_date', 'date_id'])\n",
    "\n",
    "# Drop is_missing column\n",
    "date_dim = date_dim.drop(columns=['is_missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process service dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the service dimension; we merge the domain information into the service and handle the naming alongside the importanct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge domain name into the service dimension table, adding suffixes to distinguish between columns\n",
    "dim_service = service_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left', suffixes=('_service', '_domain'))\n",
    "\n",
    "# Rename the different columns for clarity and formatting\n",
    "dim_service = dim_service.rename(columns={\n",
    "    'name_domain': 'domain_name',\n",
    "    'name_service': 'name',\n",
    "    'servicetype': 'service_type',\n",
    "    'transporttype': 'transport_type'\n",
    "})\n",
    "\n",
    "# Keep only the relevant columns\n",
    "dim_service = dim_service[['service_id', 'created_date', 'name', 'service_type', 'transport_type', 'carrier_id', 'domain_name']]\n",
    "\n",
    "# Convert to datetime preserving the time component\n",
    "dim_service['created_date'] = pd.to_datetime(dim_service['created_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process carrier and country dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the carrier and country dimensions, merging the carrier with the domain information and keeping the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the domain name into the service dimension table, adding suffixes to distinguish between columns\n",
    "dim_carrier = carrier_df.merge(domain_df[['domain_id', 'name']], on='domain_id', how='left', suffixes=('_carrier', '_domain'))\n",
    "\n",
    "# Rename some of the columns for clarity and formatting reasons\n",
    "dim_carrier = dim_carrier.rename(columns={\n",
    "    'name_carrier': 'name',\n",
    "    'name_domain': 'domain_name'\n",
    "})\n",
    "\n",
    "# Filter the important columns\n",
    "dim_carrier = dim_carrier[['carrier_id', 'name', 'created_date', 'domain_name']]\n",
    "\n",
    "# Convert to datetime preserving any time components\n",
    "dim_carrier['created_date'] = pd.to_datetime(dim_carrier['created_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the country dimension, filtering the needed columns\n",
    "dim_country = country_df[['country_id', 'name', 'isocountrycode', 'continent', 'EU']]\n",
    "\n",
    "# Here we rename the 'isocountrycode' column for clarity\n",
    "dim_country = dim_country.rename(columns={'isocountrycode': 'iso_country_code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new start schema dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last operation left to do is to merge toeghter the fact table and different dimensions into a single excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star schema transformation with domain names included completed!\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter('../../../00-Project/datasets/star_schema_dataset_1.xlsx', engine='xlsxwriter') as writer:\n",
    "    # The Fact table\n",
    "    fact_shipment.to_excel(writer, sheet_name='fact_shipment', index=False)\n",
    "    \n",
    "    # The Dimension tables\n",
    "    dim_customer.to_excel(writer, sheet_name='dim_customer', index=False)\n",
    "    dim_delivery_address.to_excel(writer, sheet_name='dim_delivery_address', index=False)\n",
    "    dim_pickup_address.to_excel(writer, sheet_name='dim_pickup_address', index=False)\n",
    "    date_dim.to_excel(writer, sheet_name='dim_date', index=False)\n",
    "    dim_service.to_excel(writer, sheet_name='dim_service', index=False)\n",
    "    dim_carrier.to_excel(writer, sheet_name='dim_carrier', index=False)\n",
    "    dim_country.to_excel(writer, sheet_name='dim_country', index=False)\n",
    "\n",
    "print(\"Star schema transformation with domain names included completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
