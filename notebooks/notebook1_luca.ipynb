{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpQ9pUSuEk7FsT5qsIfSdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BianchiLuca28/FHNW-BI-LMS/blob/main/notebooks/notebook1_luca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sCVE5bMgeckD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from scipy import stats\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "UNva8STcekbu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "DGWyXg-zeZzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31adZDuBeEA_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/__Shared/BI\""
      ],
      "metadata": {
        "id": "pw8vfdt1eOk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(folder_path + \"/preprocessed_flattened_dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zm9jZvc-egpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "mZmzGwpGk7fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Data Exploration"
      ],
      "metadata": {
        "id": "gRGHLVgOlpOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Data Overview\n",
        "print(\"Dataset Overview:\")\n",
        "display(df.head())\n",
        "\n",
        "# Summary statistics for numerical features\n",
        "print(\"\\nSummary Statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Data types and missing value counts\n",
        "print(\"\\nData Types and Missing Values:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nMissing Values Count:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "8e74I_N2lAer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Values Analysis"
      ],
      "metadata": {
        "id": "6RyNic0WlnIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Heatmap to visualize missing values\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IY9KQeSZlalI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Heatmap for Numerical Features"
      ],
      "metadata": {
        "id": "WRPFb4CXlug7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Correlation heatmap for numerical features\n",
        "plt.figure(figsize=(15, 10))\n",
        "corr_matrix = numerical_features.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c9ZW2QQ_lkt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Feature Distributions"
      ],
      "metadata": {
        "id": "zTC6c4e1l9SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distribution for each numerical feature\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.title(f\"Distribution of {feature}\")\n",
        "    plt.xlabel(feature)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5juu6ekmmEff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Box Plots for Outlier Detection"
      ],
      "metadata": {
        "id": "7RXcR4YSmCj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot for outlier detection in each numerical feature\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.boxplot(x=df[feature])\n",
        "    plt.title(f\"Box Plot for {feature} (Outlier Detection)\")\n",
        "    plt.xlabel(feature)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WZgS26NKmFpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Interaction Plots"
      ],
      "metadata": {
        "id": "IGr9Id2ImhSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair plot for numerical features (downsample to avoid overload if data is large)\n",
        "sampled_data = df.sample(5000)  # Sample a subset to avoid overloading the plot\n",
        "sns.pairplot(sampled_data[numerical_features])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lQ8v3_PPmjdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Variable Analysis"
      ],
      "metadata": {
        "id": "76FQKxeYmmJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the class distribution of the target variable\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(df['service_type'])\n",
        "plt.title(\"Class Distribution of Service Type\")\n",
        "plt.xlabel(\"Service Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LV3kfnjkmptu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Special class of the service type"
      ],
      "metadata": {
        "id": "rDrnasER3Hgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out the 'special' class cases\n",
        "special_cases = df[df['service_type'] == 'Special']"
      ],
      "metadata": {
        "id": "gXnkYxkk2t3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_cases"
      ],
      "metadata": {
        "id": "E30AlevK4V0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distribution for each numerical feature\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Display the special cases\n",
        "print(\"Special Cases Analysis:\")\n",
        "print(special_cases.describe())  # General statistics for numerical features\n",
        "print(special_cases.head())  # View the first few rows of the special cases\n",
        "\n",
        "# Plot some of the key features for visual analysis\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(data=special_cases[numerical_features])\n",
        "plt.title('Boxplot of Numerical Features for Special Service Type')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Otaie_A3ai5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the customer price of 'special' cases with all other classes\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='service_type', y='customer_price', data=df)\n",
        "plt.title('Comparison of Customer Price Across Service Types')\n",
        "plt.xlabel('Service Type')\n",
        "plt.ylabel('Customer Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yWn2aPN254pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataframe with derived margin\n",
        "df_margin_analysis = df[['service_type', 'customer_price', 'final_carrier_price', 'expected_carrier_price']]\n",
        "\n",
        "# Derive margin estimate as customer_price - final_carrier_price or expected_carrier_price if final_carrier_price is not available\n",
        "df_margin_analysis['derived_margin'] = df_margin_analysis['customer_price'] - df_margin_analysis['final_carrier_price'].fillna(df_margin_analysis['expected_carrier_price'])"
      ],
      "metadata": {
        "id": "uMIWhVbv7Dnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the derived margin of 'special' cases with all other classes\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='service_type', y='derived_margin', data=df_margin_analysis)\n",
        "plt.title('Comparison of Derived Margin Across Service Types')\n",
        "plt.xlabel('Service Type')\n",
        "plt.ylabel('Derived Margin')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3thMD3vY7hSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance Analysis (initial)\n",
        "\n",
        "This code crashes since it performes \"get_dummies\". Change it as the next one"
      ],
      "metadata": {
        "id": "ex3YlBr2mwUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Downsample the data to avoid memory overload\n",
        "df_sampled = df.sample(frac=0.1, random_state=42)  # Use only 10% of the data\n",
        "\n",
        "# Separate the target column before any transformations\n",
        "target_column = 'service_type'\n",
        "y = df_sampled[target_column]\n",
        "\n",
        "# Drop the target column from the feature set\n",
        "X = df_sampled.drop(columns=[target_column])\n",
        "\n",
        "# Split numerical and categorical columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Handle categorical columns\n",
        "for col in categorical_cols:\n",
        "    if X[col].nunique() > 10:  # High cardinality threshold (e.g., > 10 unique values)\n",
        "        # Apply Label Encoding for high-cardinality features\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "    else:\n",
        "        # Apply One-Hot Encoding for low-cardinality features\n",
        "        X = pd.get_dummies(X, columns=[col], drop_first=True)\n",
        "\n",
        "# Encode categorical target column if not already encoded\n",
        "if y.dtype == 'object':\n",
        "    le_target = LabelEncoder()\n",
        "    y = le_target.fit_transform(y)\n",
        "\n",
        "# Train a RandomForestClassifier for feature importance analysis\n",
        "rf = RandomForestClassifier(n_estimators=20, random_state=42)  # Reduced number of estimators to limit memory usage\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Plot feature importances\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)[:10]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=importances, y=importances.index)\n",
        "plt.title(\"Top 10 Feature Importances (Initial Analysis)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "by_YNoUZm2RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation with Target Variable"
      ],
      "metadata": {
        "id": "HP0QErxMnIW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plots to see how numerical features relate to the target variable\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x=df['service_type'], y=df[feature])\n",
        "    plt.title(f\"Relationship between {feature} and Service Type\")\n",
        "    plt.xlabel(\"Service Type\")\n",
        "    plt.ylabel(feature)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-5qwlEOQnK0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ds8pYvk_gQoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing unavailable columns at the moment of the prediction"
      ],
      "metadata": {
        "id": "h_NmbrDvelo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_exclude = [\n",
        "    'year_delivery_date', 'month_delivery_date',\n",
        "    'quarter_delivery_date', 'year_real_delivery_date',\n",
        "    'month_real_delivery_date', 'quarter_real_delivery_date',\n",
        "    'final_carrier_price',\n",
        "    'margin', 'lms_plus',\n",
        "    'year_pickup_date', 'month_pickup_date', 'quarter_pickup_date',\n",
        "    'year_real_pickup_date', 'month_real_pickup_date', 'quarter_real_pickup_date',\n",
        "    'domain_name_service',\n",
        "    'domain_name_customer',\n",
        "    'transport_type', 'shipment_id',\n",
        "    'name_service'\n",
        "]"
      ],
      "metadata": {
        "id": "tdDZNdb_eroJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the identified columns to prevent label leakage\n",
        "df = df.drop(columns=columns_to_exclude, axis=1)"
      ],
      "metadata": {
        "id": "GqFcbBCye3zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing values"
      ],
      "metadata": {
        "id": "b7xbs-Qzg6hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# columns with NAs (with more than 0)\n",
        "df.isna().sum()[df.isna().sum() > 0]"
      ],
      "metadata": {
        "id": "z1M_scQkgSd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with too many missing values (more than 70% missing)\n",
        "threshold = len(df) * 0.7\n",
        "df = df.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "# Fill numeric columns with median and categorical columns with 'missing'\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "df[categorical_cols] = df[categorical_cols].fillna('missing')"
      ],
      "metadata": {
        "id": "mJ---FkcgtpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Encoding"
      ],
      "metadata": {
        "id": "GF5X93v-HFNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Encode Categorical Target\n",
        "target_column = 'service_type'\n",
        "le_target = LabelEncoder()\n",
        "df[target_column] = le_target.fit_transform(df[target_column])"
      ],
      "metadata": {
        "id": "KovefjqJor6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Features and Target\n",
        "X = df.drop([target_column], axis=1)\n",
        "y = df[target_column]"
      ],
      "metadata": {
        "id": "Srm-ciD-ovPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding Strategy for Features\n",
        "# Split categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Apply Encoding to Categorical Features\n",
        "# Label Encoding for high-cardinality features, One-Hot Encoding for low-cardinality features\n",
        "for col in categorical_cols:\n",
        "    if X[col].nunique() > 10:  # High cardinality threshold (e.g., >10 unique values)\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "    else:\n",
        "        X = pd.get_dummies(X, columns=[col], drop_first=True)"
      ],
      "metadata": {
        "id": "aUD0iGo3oyzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Split, Outliers, SMOTE, and Scaling"
      ],
      "metadata": {
        "id": "mVBCcdTuAT-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "def custom_train_test_preprocess(\n",
        "    X, y, special_class=\"Special\", max_synthetic=200, test_size=0.2,\n",
        "    threshold=3, random_state=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Custom function to preprocess data:\n",
        "    1. Train-test split ensuring at least 3 cases of the special class in the test set.\n",
        "    2. Remove outliers from numerical features in the training set (excluding the special class).\n",
        "    3. Apply SMOTE to the training set only for the special class.\n",
        "    4. Scale numerical features in both training and test sets.\n",
        "\n",
        "    Parameters:\n",
        "        X (pd.DataFrame): Features dataset.\n",
        "        y (pd.Series): Target variable.\n",
        "        special_class (str): The class for which SMOTE is applied.\n",
        "        max_synthetic (int): Maximum number of synthetic samples for the special class.\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "        threshold (float): Z-score threshold for outlier detection.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        X_train_resampled, X_test_scaled, y_train_resampled, y_test: Preprocessed data.\n",
        "    \"\"\"\n",
        "    # Step 1: Train-test split with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    # Ensure at least 3 samples of the special class in the test set\n",
        "    special_test_count = sum(y_test == special_class)\n",
        "    if special_test_count < 3:\n",
        "        special_indices = y[y == special_class].index\n",
        "        remaining_needed = 3 - special_test_count\n",
        "\n",
        "        # Add required samples from training to test set\n",
        "        additional_test_indices = X_train.loc[special_indices].index[:remaining_needed]\n",
        "        X_test = pd.concat([X_test, X_train.loc[additional_test_indices]], axis=0)\n",
        "        y_test = pd.concat([y_test, y_train.loc[additional_test_indices]], axis=0)\n",
        "\n",
        "        # Remove these samples from the training set\n",
        "        X_train = X_train.drop(additional_test_indices)\n",
        "        y_train = y_train.drop(additional_test_indices)\n",
        "\n",
        "    # Step 2: Remove outliers from non-special cases in training set\n",
        "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "    special_mask = y_train == special_class\n",
        "    X_train_special = X_train[special_mask]\n",
        "    X_train_non_special = X_train[~special_mask]\n",
        "    y_train_special = y_train[special_mask]\n",
        "    y_train_non_special = y_train[~special_mask]\n",
        "\n",
        "    z_scores = np.abs(stats.zscore(X_train_non_special[numerical_cols]))\n",
        "    outlier_filter = (z_scores < threshold).all(axis=1)\n",
        "    X_train_non_special = X_train_non_special[outlier_filter]\n",
        "    y_train_non_special = y_train_non_special[outlier_filter]\n",
        "\n",
        "    # Combine special and non-special data back together\n",
        "    X_train = pd.concat([X_train_special, X_train_non_special], axis=0)\n",
        "    y_train = pd.concat([y_train_special, y_train_non_special], axis=0)\n",
        "\n",
        "    # Step 3: Apply SMOTE to the training set for the special class\n",
        "    smote = SMOTE(sampling_strategy={special_class: min(max_synthetic, 200)}, random_state=random_state)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Step 4: Scale numerical features\n",
        "    numerical_cols_train = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "    numerical_cols_test = X_test.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_resampled[numerical_cols_train] = scaler.fit_transform(X_train_resampled[numerical_cols_train])\n",
        "    X_test[numerical_cols_test] = scaler.transform(X_test[numerical_cols_test])\n",
        "\n",
        "    return X_train_resampled, X_test, y_train_resampled, y_test"
      ],
      "metadata": {
        "id": "cquXG65QAxqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = custom_train_test_preprocess(\n",
        "    X, y, special_class=\"Special\", max_synthetic=200, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "wZsY2fUK_fPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection"
      ],
      "metadata": {
        "id": "kDpKQgw7hs0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Feature Selection\n",
        "# Train a RandomForestClassifier to determine feature importance\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)  # Reduced number of estimators for simplicity\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get Feature Importances and Select Important Features\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
        "important_features = feature_importances[feature_importances > 0.01].index.tolist()  # Adjust threshold as needed\n",
        "\n",
        "# Create Final Dataset with Selected Features\n",
        "X_selected_train = X_train[important_features]\n",
        "X_selected_test = X_test[important_features]\n",
        "\n",
        "# Output the Selected Features\n",
        "print(\"Selected Features:\", important_features)"
      ],
      "metadata": {
        "id": "GDKBRykyhBcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training & Evaluation"
      ],
      "metadata": {
        "id": "CWAG29dBDy3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# List of models to train\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', multi_class='multinomial'),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, scale_pos_weight=1)  # scale_pos_weight used to handle imbalance\n",
        "}"
      ],
      "metadata": {
        "id": "qIJZ0Ot1D3ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and test data\n",
        "X_train, X_test = X_selected_train, X_selected_test\n",
        "y_train, y_test = y_train, y_test"
      ],
      "metadata": {
        "id": "cw4kTKOg5nnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store the results\n",
        "results = {}\n",
        "\n",
        "# Train each model in a loop and evaluate its performance\n",
        "for model_name, model in models.items():\n",
        "    # Cross-Validation for Robust Evaluation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    print(f\"{model_name} Cross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Predict probabilities if possible\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_pred_prob = model.predict_proba(X_test)\n",
        "        # Check if it's a binary or multiclass problem\n",
        "        if y_pred_prob.shape[1] == 2:  # Binary classification\n",
        "            y_pred_prob = y_pred_prob[:, 1]\n",
        "        else:  # Multiclass, calculate ROC AUC for all classes\n",
        "            y_pred_prob = y_pred_prob\n",
        "    else:\n",
        "        y_pred_prob = None\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Calculate ROC AUC if possible\n",
        "    if y_pred_prob is not None and len(set(y_test)) == 2:\n",
        "        # Binary ROC AUC\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'Receiver Operating Characteristic for {model_name}')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.show()\n",
        "    elif y_pred_prob is not None and len(set(y_test)) > 2:\n",
        "        # Multiclass ROC AUC\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
        "        print(f\"Multiclass ROC AUC: {roc_auc:.4f}\")\n",
        "    else:\n",
        "        roc_auc = None\n",
        "\n",
        "    # Store the results for comparison\n",
        "    results[model_name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Cross-Validation Accuracy\": cv_scores.mean(),\n",
        "        \"Cross-Validation Std Dev\": cv_scores.std(),\n",
        "        \"Classification Report\": class_report,\n",
        "        \"Confusion Matrix\": confusion,\n",
        "        \"ROC AUC\": roc_auc\n",
        "    }\n",
        "\n",
        "    # Print the performance metrics for the model\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
        "    if roc_auc is not None:\n",
        "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion)\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "fOZrRa2t5wIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare results across models\n",
        "# Convert the metrics to a DataFrame for easier visualization\n",
        "comparison_data = []\n",
        "for model_name, metrics in results.items():\n",
        "    comparison_data.append([\n",
        "        model_name,\n",
        "        metrics[\"Accuracy\"],\n",
        "        metrics[\"Cross-Validation Accuracy\"],\n",
        "        metrics[\"Cross-Validation Std Dev\"],\n",
        "        metrics[\"ROC AUC\"]\n",
        "    ])\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data, columns=[\"Model\", \"Accuracy\", \"Cross-Validation Accuracy\", \"Cross-Validation Std Dev\", \"ROC AUC\"])\n",
        "\n",
        "print(\"Comparison of Model Performance:\")\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "jChAx3GA5zu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare results across models\n",
        "# Convert the metrics to a DataFrame for easier visualization\n",
        "comparison_data = []\n",
        "for model_name, metrics in results.items():\n",
        "    comparison_data.append([\n",
        "        model_name,\n",
        "        metrics[\"Accuracy\"],\n",
        "        metrics[\"Cross-Validation Accuracy\"],\n",
        "        metrics[\"Cross-Validation Std Dev\"],\n",
        "        metrics[\"ROC AUC\"]\n",
        "    ])\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data, columns=[\"Model\", \"Accuracy\", \"Cross-Validation Accuracy\", \"Cross-Validation Std Dev\", \"ROC AUC\"])\n",
        "\n",
        "print(\"Comparison of Model Performance:\")\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "eivDZmSTpsD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning for XGBoost"
      ],
      "metadata": {
        "id": "y6nfwFH-q66a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# import numpy as np\n",
        "# import xgboost as xgb\n",
        "\n",
        "# # Define the XGBoost model\n",
        "# xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, scale_pos_weight=1)\n",
        "\n",
        "# # Define the parameter grid for hyperparameter tuning\n",
        "# param_grid = {\n",
        "#     'n_estimators': [50, 100, 200],\n",
        "#     'learning_rate': [0.01, 0.05, 0.1],\n",
        "#     'max_depth': [3, 5, 7],\n",
        "#     'min_child_weight': [1, 3, 5],\n",
        "#     'subsample': [0.6, 0.8, 1.0],\n",
        "#     'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "# }\n",
        "\n",
        "# # Use GridSearchCV to search for the best combination of hyperparameters\n",
        "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "\n",
        "# # Fit GridSearchCV on the training data\n",
        "# grid_search.fit(X_selected_train, y_train)\n",
        "\n",
        "# # Get the best parameters and best score from the grid search\n",
        "# best_params = grid_search.best_params_\n",
        "# best_score = grid_search.best_score_\n",
        "\n",
        "# print(\"Best Parameters from Grid Search:\", best_params)\n",
        "# print(\"Best Cross-Validation Accuracy from Grid Search:\", best_score)"
      ],
      "metadata": {
        "id": "ccSdRZl0q_SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train the XGBoost model using the best parameters\n",
        "# best_xgb_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss', random_state=42, scale_pos_weight=1)\n",
        "# best_xgb_model.fit(X_selected_train, y_train)\n",
        "\n",
        "# # Make predictions on the test set\n",
        "# y_pred_best = best_xgb_model.predict(X_selected_test)\n",
        "\n",
        "# # Evaluate the tuned model's performance\n",
        "# accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "# classification_report_best = classification_report(y_test, y_pred_best)\n",
        "# confusion_matrix_best = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "# print(f\"Test Set Accuracy (Tuned XGBoost): {accuracy_best:.4f}\")\n",
        "# print(\"Classification Report (Tuned XGBoost):\")\n",
        "# print(classification_report_best)\n",
        "# print(\"Confusion Matrix (Tuned XGBoost):\")\n",
        "# print(confusion_matrix_best)"
      ],
      "metadata": {
        "id": "Kzh8cOcPrDr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of Results\n",
        "\n",
        "### Summary of Business Impact\n",
        "\n",
        "The Random Forest model provides key insights for optimizing delivery services:\n",
        "1. **Feature Importance**: The top features driving the predictions indicate which factors most impact the service type, providing insights into areas that can be optimized.\n",
        "2. **Cost Matrix**: By analyzing the cost associated with false positives and false negatives, the model helps in understanding the financial implications of incorrect predictions. For example, reducing false positives might lower operational costs.\n",
        "3. **Operational Efficiency**: By accurately predicting the service type, resources can be allocated more effectively, improving overall efficiency.\n",
        "4. **Customer Targeting**: The insights from feature importance can help target specific customer segments with tailored services, increasing satisfaction and loyalty."
      ],
      "metadata": {
        "id": "j7l_ZTsTrO7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Train the final Random Forest model\n",
        "final_rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "final_rf.fit(X_selected_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = final_rf.predict(X_selected_test)\n",
        "y_pred_prob = final_rf.predict_proba(X_selected_test)"
      ],
      "metadata": {
        "id": "ReDgOgbecAvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "confusion = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "TpJeBWCfdkEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROC AUC for multiclass\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
        "\n",
        "# Plot ROC Curve (for each class)\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(len(final_rf.classes_)):\n",
        "    fpr, tpr, _ = roc_curve(y_test == i, y_pred_prob[:, i])\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'Class {i} (area = {auc(fpr, tpr):.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Random Forest (Multiclass)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "soZoz93Bdnv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=final_rf.classes_, yticklabels=final_rf.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xu0X2MOddqrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost Matrix Analysis\n",
        "# Define a cost matrix (example values, can be adjusted based on business context)\n",
        "cost_matrix = np.array([[0, 10], [5, 0]])  # [[TN cost, FP cost], [FN cost, TP cost]]\n",
        "\n",
        "# Calculate cost based on confusion matrix for multiclass\n",
        "cost = 0\n",
        "for i in range(len(confusion)):\n",
        "    for j in range(len(confusion)):\n",
        "        cost += confusion[i, j] * cost_matrix[min(i, 1), min(j, 1)]\n",
        "print(f\"Total Cost Based on Cost Matrix: {cost}\")"
      ],
      "metadata": {
        "id": "gxWn7M-Ydxac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Cost Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cost_matrix, annot=True, cmap='Reds', fmt='g', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Cost Matrix Visualization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_bRrQB8De0WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance Analysis\n",
        "importances = pd.Series(final_rf.feature_importances_, index=X_selected_train.columns).sort_values(ascending=False)[:10]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=importances, y=importances.index)\n",
        "plt.title(\"Top 10 Feature Importances in Random Forest Model\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NiQKvS47d0Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Distribution Analysis\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(y_pred, palette=\"viridis\")\n",
        "plt.title('Distribution of Predicted Classes')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(y_test, palette=\"viridis\")\n",
        "plt.title('Distribution of True Classes')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wmeWxU2JfwXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Business Interpretation\n",
        "print(\"\\nBusiness Interpretation of Results:\")\n",
        "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "for label, metrics in class_report.items():\n",
        "    if isinstance(metrics, dict):\n",
        "        print(f\"{label}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}\")"
      ],
      "metadata": {
        "id": "iBnNmsu-d1bI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}